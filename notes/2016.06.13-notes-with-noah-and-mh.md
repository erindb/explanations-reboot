* computational psycholinguistics
	* give a meaning to "A because B" sentences that works
	* work out the psychology and linguistics of that
	* design an experimental paradigm that gets interesting explanations
		- model has some parameters:
			- stickiness parameter
			- does not A have some radius? how far do we need to be from A?
		- note on cause VS because from L.A. Paul: cause has to be events, but because does not
		- naturalistic domains or artificial domains?
		- if people are explaining, then they need to be experts
		- well-known domains:
			* block world?
			* grid world?
			* tic-tac-toe
		- components:
			* objects w/ physics/causal properties
			* kinds of objects
			* agents
			* sequences of events
		- abstract components:
			- artifacts
			- agents
			- kinds
			- games
		- covariation?
		- maybe parameter learning is OK
			* teeter totter approach
			* same boy or different one
			* did she jump because ...
		- stimuli
			* short movie clips?
			* why A?
			* A because ____
	* experiment:
		- design
			- some people and some physical events
			- identify events
			- why event?
			- event because _____.
		- questions
			* why A?
			* did A happen?
			* what will happen next?
		- goal
			* relate these different questions with the latent intuitive theory variable
			* we're not hypothesizing a meaning. explanation is too senstivie to intutive theories, which are super complicated.
			* there's an intuitive theory and answers to questions. how are the answers to these questions related to the intuitive theory (which is not directly observable)
* learning from explanations
	* machine learning (start here)
		* suppose we had a bunch of "A because B" data. Can we use that to learn a model of the world? And is it better than the individual propositions themselves.
		* choices of what kind of model are we trying to learn?
			- assume intuitive theory is something structured like a prob prog
			- assume a totally unstructured forward model (variational autoencoder)
				- can we cache out the explanations in terms of potential observations, we can do counterfactuals
	* psychology

* getting probabilistic programming systems to explain themselves (computer science-y)
	* we have a probabilistic model
	* we do a specific inference
	* we want not just an answer, but also *why* it gave that answer.
	* normally we just return a distribution over that value, but we would instead return Value + Explanation(Value)
	* what could the explanations be
		- the data
		- the latent variables in the model
		- the model
			* look at prior over what models people write
	* what domains?
		- topic model
		- agents, properties, etc.