informativity is better
because composition would be possible
nested inference
utility of naming the difference maker: explanation as blame (tobi)
condition on counterfactual, update some belief distribution
so l0 has to be uncertain about something.
explanations are about structural assumptions
L0 who has model uncertainty: and that's the QUD
lifted semantics: lifted is possible explanations
L0 can measure CF difference: that's the scale.
what belief update does that cause in the L0?
what does L0 have uncertainty about?
in the case of regression maybe it's clearer?
why A?
	- what's the uncertainty a questioner could have that would cause them to ask that?
when will people ask "Why A?" vs. "A?" "B?" or staying silent
	- when they're not sure which of a set of causes matters
	- they're not sure about causal connections
"A because B" means different from "A and B". How do we capture that?
It's sensible to use CF as literal meaning if there's parameter uncertainty about the model.
write out all the kinds of uncertainty that L0 could have. which would give rise to the S1 utility?
class of models with uniform uncertainty?
just consider because sentences
think through where explanation updates multiple variables at once
intuition: is creating L0 uncertainty similar to creating counterfactual worlds?
intuition: is the speaker utility model somehow a closed form of the L0/S1 model under some assumptions?
