Situation:

B_____________C
|             |
|             |
|             |
|             |
A_____________D

C1=1
C2=1

Rationality=1

Choice=B

==========================

CF Prior:

C1 ~ Unif([0,1])
C2 ~ Unif([0,1])

A,B,C,D ~ unif([ (0,0), (0,1), (1,0), (1,1) ])

util(A) = C1*f1(A) + C2*f2(A)
....

P(Choice=A) ‚àù util(A)

==========================

we can explain things iff they could have been different under our CF prior.

we can offer and explanation iff it could have been different under our CF prior.

explanations are good when counterfactually changing the explanans leads to resampling the explanandum.

How do we construct a CF prior so that...
* the things we want to explain could have been different
* the things that could be explanations could have been different
* counterfactually changing the explanans would also change the explanandum

==========================

1. construct an actual situation

2. ask about something: Y

3. construct CF prior s.t. Y might have been different

4. any values with alternatives are candidate explanations

5. best explanation is best counterfactual difference-maker

==========================

Agents do stuff through time, so it would be nice to start to model that. Even if the context the agent is in doesn't vary with time.
