* we have a system that already does a thing
* what's a good explanation of that
* if data varies, we can point to data
* agent-like system that forms beliefs and makes decisions
	- i did this because i thought that (latent variables)
	- because i wanted (utility function)
* having complex beliefs
	- the closer we get to that, the better the explanations might feel
* if you have a system that's structured like that, it's useful.
	- but even if we don't have that structure, we could still potentially create a structure for the sake of explanation
* homework: implement an explanatory version of some typical learning algorithm
* what do we revise in the counterfactual
* for a deterministic variable, there are no latent variables. so we either have to talk about the input or the program.
* explanatory debugger
	- scrape github for a ton of code in some language to learn a prior over programs
	- learn counterfactuals over program structure
	- don't work with the raw source code
	- work with control flow analysis
	- there must be distributions over labeled graphs. and that would be our prior.
	- this contruct of a prior over programs and learning from data
		* it's never been clear how this would be useful.
		* inductive programming is hard.
		* but counterfactuals are slightly more easy.
		* careful, edited programs need not almost-always throw errors.
		* alex akin and patixa (sp?) interested in inductive programming
* step 1:
	* assume probabilistic program, leave structure alone
	* explain in terms of observations (and latent variables)
	* implement the systems that will do counterfactuals and come up with explanations
	* NN would permit just data basically
	* more structured models would permit latent variables
	* spec 0:
		- hand in observations and actions it can take
		- it has prob prog and inference, posterior dist over some predictive things. you can condition on data and form beliefs.
		- you hand it a utility function and it softmaxes its utlity. so you can have it explain its actions.
		- bayesian one-shot decision agent
	* spec 1:
		- hand in prob prog and data
		- it predicts stuff
		- have it explain its predictions
		- S1 model with a bunch of ways of talking about latent variables and data
		- because this variable was likely this value, or because this variable was possibly this value
		- speaker needs to evaluate literal listener.
			- literal: take the program and the counterfactual assertions and evaluate them
		- propose a few different specs for the interface and how those would be implemented. note: there's currently a macro pass. ask paul where does macro transformation happen.
		- directive: make this block of code counterfactualizable, and this has the hooks we need for the augmented model
		- **think through this archetecture**
	* **write down 3-4 case-study programs**
		* trained regression to make prediction
		* latent random variables, e.g. mixture model
			- it can exlain in terms of inferred category membership
		* something interesting that would be fun.