We learned something.
People explain an intentional action via a goal or desire.
	* the beliefs that the person has are clear and not atypical
	* simulate
		- beliefs
		- goals
		- what are the assumptions about priors over beliefs and goals that lead to this effect
		- that leads into: the best explanation for intentional action is often a goal
		- explaining something with a typical desire is better than explaining something with a typical belief.
		- maybe we just expect more diversity in desires
Do we want to focus on explanations of actions? (read Desmond's paper) Or do we just want non-action/agent things.

PrExTh:
	* most people are bad at explaining linguistics
	* computational social science
	* Gelman style
	* take census data and build regression model of it
	* why vote for this candedate? because the model says this.
	* more data science-y data
	* predictors of income
	* something more social and intuitive
	* desmond's experiment with the spinner
	* a domain with some data where it actually is linear-ish
	* bob failed the exam. because the exam was hard, or because he didn't study enough?
	* maybe this is the wrong question, though.
	* say you know the structure of the regression but not the weights.
	* how satisfying an explanation is this?
	* noisiness / trueness / common knowledge ?
	* what good is an explanation (for export -tania)
	* do people make better predictions
	* situations with asymetric domain knowledge are probably more interesting anyway ---look at controversial issues
	* training set and test set. normally we train and report accuracy on test.
		- what if you did automated error analysis.
		- find errors on test set and *why* it made those predictions.
	* darpa
		- prediction, classification, multimedia classification
			* logistic regression
		- autonomous vehicle sees stuff and comes back and explains what it did and why
			* action with video with kids
			* automated auditing of UAVs
			* minirobot models
			* our robot can appeal to factors
			* the utility of the robot is a linear function of the world
			* the thing will take actions that will increase its utility
				- and the utility is exactly what you want to fix in a robot.
				- the structure of the utility:
					* not "in this world my utility was higher for this world than that world" but rather "because this was my goal"
	* both prongs are regressions
		- one softmax utility
		- one logistic classifier
* strategy:
	- generate explanations from this space of models
	- apply to simple maybe artificial models