---
title: "All booleans! Vary structure!"
author: "Erin Bennett"
output: 
html_document:
toc: false
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo=F, warning=F, cache=T, message=F, sanitiz =T, fig.width = 5, fig.height = 3)
```

```{r load_settings}
# install.packages('devtools')
# devtools::install_github("mhtess/rwebppl")
source("~/Settings/startup.R")
library(reshape2)
theme.new = theme_set(theme_bw(12))
```

## 1. Using only boolean variables, do counterfactual probabilities match up with L&K's model?

```{r}
counterfactual = function(model, variable, storynumber,
                          premise.variable, premise.value) {
  program_file = paste(model, "/lk", storynumber,
                       "/autoexpanded.wppl", sep="")
  rs = webppl(program_file = program_file,
              inference_opts = list(method="enumerate"),
              model_var = paste(
                "function() {return counterfactual(0.53, {'",
                premise.variable,
                "': ",
                ifelse(premise.value, "true", "false"),
                "});}",
                sep=""),
              packages = "./node_modules/jsUtils")
  return(sum(rs[rs[[variable]]==T, "prob"]))
}
```

```{r}
lk.model = read.csv("lk-model-data-numbers.csv") %>%
  mutate(my.model = mapply(counterfactual, "rsa.base.booleans", variable,
                           story, premise.variable, premise.value),
         story = factor(story))

lk.model %>% ggplot(., aes(x=rating, y=my.model, colour=story)) +
  geom_point() +
  scale_colour_brewer(type="qual", palette=2) +
  #theme_black +
  #theme(legend.position="right") +
  ylab("my code") +
  xlab("their code")
#ggsave("model-replication.png", width=5, height=3)
```

Yep.

## 2. What about explanations?

We have human ratings for some explanations. In each story from L&K, we have two explanations that share an explanans (e.g. "B because A" and "C because A").

```{r}
# expl = read.csv("explanations-expt-data.csv")
# expl %>% filter(story=="story1") %>%
#   mutate(
#     variable = factor(variable),
#     utterance = factor(
#       variable, levels=c("A", "B"),
#       labels=c("A because A", "B because A"))) %>%
#   ggplot(., aes(x=utterance, y=rating)) +
#   geom_point(size=2, colour="white") +
#   geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, lwd=1, colour="white") +
#   ylim(0, 1) + theme_black +
# theme(legend.position="right")
# ggsave("explanations-results-story1.png", width=5, height=3)
```


```{r, fig.height=5}
# expl = read.csv("explanations-expt-data.csv")
# expl %>% mutate(tag = ifelse(tag %in% c("higher", "lower"), "miss", as.character(tag))) %>%
# ggplot(., aes(x=variable, y=rating, colour=tag)) +
#   geom_point(size=2) +
#   geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, lwd=1) +
#   facet_wrap(~story, scale="free") +
#   scale_colour_brewer(type="qual", palette = 2) +
#   ylim(0, 1) + theme_black +
# theme(legend.position="right")
# ggsave("explanations-results-tagged.png", width=12, height=8)
```

```{r, fig.height=5}
# expl = read.csv("explanations-expt-data.csv")
# ggplot(expl, aes(x=variable, y=rating)) +
#   geom_point(size=2, colour="white") +
#   geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, lwd=1, colour="white") +
#   facet_wrap(~story, scale="free") +
#   ylim(0, 1) + theme_black +
# theme(legend.position="right")
# ggsave("explanations-results.png", width=10, height=8)
```

```{r, fig.height=5}
expl = read.csv("explanations-expt-data.csv")
ggplot(expl, aes(x=variable, y=rating)) +
  geom_point(#colour="white", 
    size=2) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, lwd=1#, colour="white"
                ) +
  # geom_point(colour="#e7298a", size=2) + geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, lwd=1, colour="#e7298a") +
  facet_wrap(~story, scale="free") +
  ylim(0, 1) #+ theme_black +
  # theme(legend.position="right")
# ggsave("explanations-results.png", width=10, height=8)
```

This display is a bit difficult to interpret, because each story corresponds to its own causal structure and each is also associated with a specific configuration of what is true of the "actual world". Here's a sketch of the stories that were featured in this experiment:

![](lkstories.png)

Later, I'll go story-by-story and explanation-by-explanation to try to understand the data and compare them to the predicitons of various models.

Let's first take a simple model of explanations as speech acts, and compare it to the data that we have.

### First Pass Model

There are different ways to do a model of explanation, and the whole point of this exercise is to ultimately come up with a really good one.

For now, let's say that all utterances are equally probable. (We could later add in a cost for the length of an utterance, but let's leave that for much later.)

At first, let's try having a *bunch* of alternative utterances, including "and" statements and simple statements of a single variable's value. (We might want to narrow this down to only explanations. Or, we might want to narrow this down even further to explanations whose component expressions are true in the world.)

Finally, we want all of the variables in the model to be "observable", including the variables that say whether or not the causal relation between truly observable variables was "enabled" in the actual world, or otherwise modify one variable's ability to influence another.

We should check each stage of this model, so we know that the model is doing what I think it's doing!

#### Utterance Prior

So let's start with the utterance prior for each story. I'm only going to graph a couple of the "explanations only" version of the prior, but the other utterance priors look OK.

```{r}
# rs = webppl(program_file = paste("rsa.base.booleans/lk1/autoexpanded.wppl", sep=""),
#                 inference_opts = list(method="enumerate"),
#                 model_var = "possiblePriors['only_explanations']",
#                 packages = "./node_modules/jsUtils")
# rs
```

```{r}
utterancePriors = rbind(
  do.call(rbind, lapply(1:6, function(storynumber) {
    ## return utterance prior data frame
    model = "rsa.base.booleans"
    rs = webppl(program_file = paste(model, "/lk", storynumber,
                                     "/autoexpanded.wppl", sep=""),
                inference_opts = list(method="enumerate"),
                model_var = "possiblePriors('all_alternatives')",
                packages = "./node_modules/jsUtils")
    rs$story = paste("story", storynumber)
    rs$storynumber = storynumber
    rs$alternatives = "all"
    return(rs)
  })),
  do.call(rbind, lapply(1:6, function(storynumber) {
    ## return utterance prior data frame
    model = "rsa.base.booleans"
    rs = webppl(program_file = paste(model, "/lk", storynumber,
                                     "/autoexpanded.wppl", sep=""),
                inference_opts = list(method="enumerate"),
                model_var = "possiblePriors('only_explanations')",
                packages = "./node_modules/jsUtils")
    rs$story = paste("story", storynumber)
    rs$storynumber = storynumber
    rs$alternatives = "explanations"
    return(rs)
  }))
)
plot_story= function(strynum) {
  utterancePriors %>%
    filter(alternatives=="explanations") %>%
    filter(story==paste("story", strynum)) %>%
    ggplot(., aes(x=support, y=prob)) +
    geom_bar(stat="identity") +
    facet_wrap(~ story, scale="free") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
plot_story(1)
plot_story(2)
plot_story(4)
```

#### Literal Listener

Next, let's check the literal interpretations of some "because" sentences (and some other sentences).

The semantics I'm choosing for "because" for now is one where "X because Y" does not *explicitly* imply "X" and "Y". (We might want to change that later: the few linguists I've talked to so far mostly think that "X" is presupposed but that "Y" is entailed.)

I'm assuming that a literal listener calculates the probability of the counterfactual corresponding to the because statement for each world and factors according to that probability. So if the counterfactual probability is especially for a particular world, that world will be downweighted from the prior.

##### Story 1: no cause

![](lkstories-graphs/story1.png)

```{r}
literal1 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk1/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs1 = rbind(literal1("''"), literal1("'A'"), literal1("'A because A'"), literal1("'B because A'"))
```

```{r}
print_world = function(a, b) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", "")
  ))
}
rs1 %>% mutate(world = mapply(print_world, output.A, output.B)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

Under this semantics, for story 1, "A because A" provides no information, and "B because A" provides a tiny bit of information: "B" is more likely to be false. (That way, if "A" were false in a counterfactual, "B" would also happen to be false, because it was false in the actual world.) In contrast to the information provided by the utterance "A" this is a really, really tiny belief update.

##### Story 2: Causal chain

![](lkstories-graphs/story2.png)

Story 1 doesn't *actually* have any cause in it. Let's look at a story that does have cause. In story 2, A causes B and B causes C. So let's try the same sentences: "", "A", "A because A", and "B because A".

```{r}
literal2 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk2/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs2 = rbind(literal2("''"), literal2("'A'"), literal2("'A because A'"), literal2("'B because A'"))
```

```{r}
print_world = function(a, b, ab, bc) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", ""),
    ifelse(ab, "ab", ""),
    ifelse(bc, "bc", "")
  ))
}
rs2 %>% mutate(world = mapply(print_world, output.A, output.B, input.ab, input.bc)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

```{r}
rs2 %>% mutate(world = mapply(print_world, output.A, output.B, input.ab, input.bc)) %>%
  filter(utterance != "'A'") %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

In story2, with the semantics we're using and no causal uncertainty, "B because A" conveys no information. "B because A" and "C because B" also convey no meaning.

This makes total sense. In this cause structure, "B" can be true if and only if both "A" and "ab" are true. So when we're evaluating the truth of "B because A," which in our semantics is equivalent to "Counterfactually, if !A, then !B," we have to condition our counterfactual world-generator on only giving us worlds where A is false. In every single one of those worlds, B is also false. So no matter what actual world state we are in, this statement is trivially true given the causal model.

We would *have* to introduce causal uncertainty in order for "B because A" to evaluate to anything but true.

Similarly, "C because B" is always true.

In contrast, the reverse direction, "A because B", actually does convey some information about the actual state of the world. Given this utterance, this world is slightly more likely:

* "A" (and consequently "B") were false, but the causal relation between them was enabled

And it's slightly less likely that "A" is true (for similar reasons to story 1).

```{r}
rs2b = rbind(literal2("''"), literal2("'A because B'"), literal2("'C because B'"))
rs2b %>% mutate(world = mapply(print_world, output.A, output.B, input.ab, input.bc)) %>%
  filter(utterance != "'A'") %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

So looking at marginal probabilities, the counterfactual utterance "If !B then !A," updates our beliefs such that it's slightly more likely that "A" (and consequently "B") was false and that the causal relation between "A" and "B" was enabled.

```{r}
rs2b %>% rename(A = output.A, B = output.B, ab = input.ab, bc = input.bc) %>%
  select(A, B, ab, bc, prob, utterance) %>%
  gather("variable", "value", c(A, B, ab, bc)) %>%
  group_by(utterance, variable) %>%
  summarise(marginal.prob = sum(prob[value])) %>%
  ggplot(., aes(x=variable, y=marginal.prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

##### Story 3: Polarity of cause

![](lkstories-graphs/story3.png)

In story 3, the variable B controls the polarity of A's causal influence on C. If B is true, then C will match the value of A. Otherwise, C will have the opposite value from A.


```{r}
literal3 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk3/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs3 = rbind(literal3("''"), literal3("'A'"),
            literal3("'C because A'"),
            literal3("'A because C'"),
            literal3("'A because B'"),
            literal3("'B because A'"),
            literal3("'C because B'"),
            literal3("'B because C'"))
```

Just like in the other stories, a direct utterance about a state of one of the variables (e.g. "A") updates beliefs much more than any of the explanations.

```{r}
print_world = function(a, b, c) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", ""),
    ifelse(c, "C", "")
  ))
}
rs3 %>% mutate(world = mapply(print_world, output.A, output.B, output.C)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

But, zooming in on explanations...

I have trouble interpreting this. It looks like the explanans variable matters, but the explanandum doesn't really make a difference. I'm not sure why that would be. 

```{r}
rs3 %>% mutate(world = mapply(print_world, output.A, output.B, output.C)) %>%
  filter(utterance != "'A'") %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

Here are the marginal probabilities for each variable for the different explanations:

```{r}
rs3 %>% 
  # filter(utterance=="''") %>%
  filter(utterance!="'A'") %>%
  rename(A=output.A, B = output.B, C = output.C) %>%
  select(A, B, C, prob, utterance) %>%
  gather("variable", "value", c(A, B, C)) %>%
  group_by(utterance, variable) %>%
  summarise(marginal.prob = sum(prob[value])) %>%
  ggplot(., aes(x=variable, y=marginal.prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  # ggtitle("marginal prob under prior") +
  theme_bw() +
  scale_fill_brewer(type="qual", palette=2)
```

I'm not sure how to interpret these belief updates due to the explanations, but it seems pretty clear (from going through a few examples) that this *is* reflecting the relevant counterfactual probabilities accurately.


##### Story 4: Competing causes

![](lkstories-graphs/story4.png)

In story 4, if either A or B is true, then both C and D are also true.

```{r}
literal4 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk4/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs4 = rbind(literal4("''"),
            literal4("'! A'"),
            literal4("'C because A'"),
            literal4("'D because C'"))
```

```{r}
print_world = function(a, b, c, d) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", ""),
    ifelse(c, "C", ""),
    ifelse(d, "D", "")
  ))
}
rs4 %>% mutate(world = mapply(print_world, output.A, output.B, output.C, output.D)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

For this causal story, explanations actually result in belief updates almost as strong as an utterance giving an explicit variable value (e.g. "C because A" is on a similar order of informativity as "!A")

"C because A" intuitively means that A was critical for C to be true. This suggests that B is false, since if it were true, A would matter less. So when the literal listener interprets "C because A", she downweights (relative to the prior) worlds where B is true and upweights worlds where B is false.

This is the first explanation that behaves in line with intuition!

"D because C", since (in the absense of structural uncertainty) this relationship is deterministic in all possible world states, is completely uninformative to the literal listener.

##### Story 5: Asymmetric competing causes

![](lkstories-graphs/story5.png)

In story 5, A and B are equally probable. In causal power over C, A dominates, such that C is true always when A is true. But even if A is false, if B is true, then C will usually be true. D is a deterministic function of C as before.

```{r}
literal5 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk5/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs5 = rbind(literal5("''"),
            literal5("'! A'"),
            literal5("'C because A'"),
            literal5("'C because B'"),
            literal5("'D because C'"))
```

```{r}
print_world = function(a, b, c, d, bc) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", ""),
    ifelse(c, "C", ""),
    ifelse(d, "D", ""),
    ifelse(bc, "bc", "")
  ))
}
rs5 %>% mutate(world = mapply(print_world, output.A, output.B, output.C, output.D, input.bc)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

"D because C" is equally uniformative as before, which we would expect without any causal uncertainty.

"C because A" reducs the probability of its alternative cause B, but "C because B" reduces the probability of *its* alternative cause A much more, since this alternative cause was a stronger one.

##### Story 6: Causal polarity

![](lkstories-graphs/story6.png)

In story 6, there are two +/- variables (Mab and Mbc) which if true indicate that the downstream variable (e.g. B) will take the same value as the upstream variable (e.g. A) and if false, indicate that it will take the opposite value.

```{r}
literal6 = function(utterance) {
  rs = webppl(program_file = "rsa.base.booleans/lk6/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return sample(literalERP(", utterance, "));}"),
              packages = "./node_modules/jsUtils")
  rs$utterance = utterance
  return(rs)
}
rs6 = rbind(literal6("''"),
            literal6("'A'"),
            literal6("'B because A'"),
            literal6("'A because B'"),
            literal6("'C because A'"))
```

```{r}
print_world = function(a, b, c, mab, mbc) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", ""),
    ifelse(c, "C", ""),
    ifelse(mab, "+", "-"),
    ifelse(mbc, "+", "-")
  ))
}
rs6 %>% mutate(world = mapply(print_world, output.A, output.B, output.C, input.Mab, input.Mbc)) %>%
  ggplot(., aes(x=world, y=prob, fill=utterance)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
# marginals6 = rs6 %>% rename(A = output.A, B = output.B, C = output.C, Mab = output.Mab, Mbc = output.Mbc) %>%
#   select(A, B, C, Mab, Mbc, prob, utterance) %>%
#   gather("variable", "value", c(A, B, C, Mab, Mbc)) %>%
#   group_by(utterance, variable) %>%
#   summarise(marginal.prob = sum(prob[value==T]))
# marginals6 %>% ggplot(., aes(x=variable, y=marginal.prob, fill=utterance)) +
#   geom_bar(stat="identity", position="dodge") +
#   scale_fill_brewer(type="qual", palette=2)
```

For this causal structure, "C because A" indicates that there is probably a matching causal relationship (+) between A and B and between B and C. "B because A" slightly increases the probability that the first causal relationship (between A and B) is matching, as does "A because B" (though A is slightly less likely to be true in this case).

##### Summary

This completes our exploration of the literal listener's belief states over worlds after hearing some explanations *without any causal uncertainty* and *without "because" implying its components*.

#### Speaker

##### Story 1: no cause

![](lkstories-graphs/story1.png)

```{r}
speaker1 = function(alternatives) {
  rs = webppl(program_file = "rsa.base.booleans/lk1/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return speaker('", alternatives, "');}", sep=""),
              packages = "./node_modules/jsUtils")
  rs$alternatives = alternatives
  return(rs)
}
rs1 = rbind(speaker1("all_alternatives"),
            speaker1("only_explanations"),
            speaker1("only_answers_to_why: A"),
            speaker1("only_answers_to_why: B"))
```

```{r, fig.width=10, fig.height=8}
rs1 %>% ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2) +
  facet_wrap(~alternatives, scale="free_x")
```

There's no cause here, intuitively true explanations convey almost no information to the model, and the explanations a speaker considers given that both variables are true are pretty nonsensical relative to what people would say *unless* we narrow down to answers to "why A?" or "why B?", in which case the consistently uniformative "A because A" and "B because B" are somehow the best explanations.

Note that so far, I haven't forced the explanans and explanandum of a "because" statement to be true.

##### Story 5: Asymmetric competing causes

![](lkstories-graphs/story5.png)

```{r}
speaker5 = function(alternatives) {
  rs = webppl(program_file = "rsa.base.booleans/lk5/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return speaker('", alternatives, "');}", sep=""),
              packages = "./node_modules/jsUtils")
  rs$alternatives = alternatives
  return(rs)
}
rs5 = rbind(speaker5("all_alternatives"),
            speaker5("only_explanations"),
            speaker5("only_answers_to_why: A"),
            speaker5("only_answers_to_why: B"),
            speaker5("only_answers_to_why: C"),
            speaker5("only_answers_to_why: D"))
```

Even when we narrow down to only explanations with the same explanandum, the explanations that get the highest ratings don't seem to be the intuitively best explanations.

```{r, fig.width=10, fig.height=8}
rs5 %>% filter(prob > 0.02) %>%
  ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2) +
  facet_wrap(~alternatives, scale="free_x") +
  ggtitle("utterances above 0.02")
```

#### Comparison to human ratings

Unsurprisingly, there is very little correlation between people's ratings of explanations and the speaker model's probability of choosing those explanations. But let's make the graphs that show that.

```{r}
# rs = webppl(
#   program_file = paste("rsa.base.booleans/lk",
#                        6,
#                        "/autoexpanded.wppl", sep=""),
#   model_var = "function() {return speaker('only_explanations');}",
#   inference_opts = list(method="enumerate"),
#   packages = "./node_modules/jsUtils")
# rs
```

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")
  
  rs = webppl(
    program_file = paste("rsa.base.booleans/lk",
                         story.number,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste("s2('",
    utterance, "', '", explanandum, "')", sep=""),
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]
  
  return(prob_utterance)
}
```


```{r}
expl.with.model = expl %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=story)) +
  geom_point() +
  ylim(0, 1) +
  xlim(0,1) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high)) +
  scale_colour_brewer(type="qual", palette = 2) + theme_black +
theme(legend.position="right")
ggsave("RSA-base.png", width=5, height=3)
# cor.test(expl.with.model$model, expl.with.model$rating)
```

But wait! When we narrow down the alternative utterances to only explanations with the same explanandum, we do much better. Our correlation is still only 0.427, but we're still maybe getting somewhere.

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")
  
  rs = webppl(
    program_file = paste("rsa.base.booleans/lk",
                         story.number,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste(
      "function() {return speaker('only_answers_to_why: ",
      explanandum,"');}", sep=""),
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]
  
  return(prob_utterance)
}
expl.with.model = expl %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=story)) +
  geom_point() +
  ylim(0, 1) +
  xlim(0,1) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high)) +
  scale_colour_brewer(type="qual", palette = 2) +
  ggtitle("with explanandum constrained") #+ theme_black +
#theme(legend.position="right")
#ggsave("RSA-base-explanandum-fixed.png", width=5, height=3)
# cor.test(expl.with.model$model, expl.with.model$rating)
```

## 3. Now let's introduce some causal uncertainty!

The biggest problem here is that explanations communicate something about causal structure, and our listener has zero uncertainty about the causal structure, so there's nothing to learn. Let's introduce some causal uncertainty.

For now, let's go utterance-by-utterance and introduce uncertainty the one causal link that's most relevant to the particular explanation we're interested in.

### Story 1: Variable A

```{r}
# ## when structural params are fixed, forward sampling works the same way
# rs.onelinkA = webppl(program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
#             model_var = "highOrderForwardSample",
#             inference_opts = list(method="enumerate"),
#             packages = "./node_modules/jsUtils")
# rs.base = webppl(program_file = "rsa.base.booleans/lk1/autoexpanded.wppl",
#             model_var = "highOrderForwardSample",
#             inference_opts = list(method="enumerate"),
#             packages = "./node_modules/jsUtils")
# rs.base
# rs.onelinkA
```

The first explanation that I want to model is "A because A" in story 1.

This is a pretty good explananation according to humans for this story, since "A because B" would suggest that A causally depends on B.

#### Counterfactual

In story1, introducing the possibility that A might take the exact same value as B raises the counterfactual probability that A and B have matching values.

```{r}
rs.onelinkA = webppl(program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
                     model_var = "counterfactual",
                     inference_opts = list(method="enumerate"),
                     packages = "./node_modules/jsUtils") %>% mutate(structure="onelink")
rs.base = webppl(program_file = "rsa.base.booleans/lk1/autoexpanded.wppl",
                 model_var = "counterfactual",
                 inference_opts = list(method="enumerate"),
                 packages = "./node_modules/jsUtils") %>% mutate(structure="base")
cf1 = rbind(rs.base, rs.onelinkA)
print_world = function(a, b) {
  return(paste(
    ifelse(a, "A", ""),
    ifelse(b, "B", "")
  ))
}
cf1 %>%
  mutate(world = mapply(print_world, A, B)) %>%
  ggplot(., aes(x=world, y=prob)) +
  geom_bar(stat="identity", position="dodge") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  facet_wrap(~structure) +
  ggtitle("counterfactual given !cause, A, B")
```

#### Interpreting counterfactuals

Under causal story1, with some causal uncertainty, what worlds are more likely given the explanations "A because A", "B because A", and "A because B"?

#### Rating explananations

When we ask for speaker ratings in this world, we get that "!B because A" and "!A because B" are the best explanations.

```{r}
speaker1 = function(alternatives, entailmentType) {
  rs = webppl(program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return speaker('",
                                alternatives, "', '",
                                entailmentType, "');}", sep=""),
              packages = "./node_modules/jsUtils")
  rs$alternatives = alternatives
  rs$entailment = entailmentType
  return(rs)
}
rs1 = rbind(
  speaker1("only_explanations", "none"),
  speaker1("only_explanations", "only_explanans"),
  speaker1("only_explanations", "both"))
```

```{r, fig.width=10}
rs1 %>% ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2) +
  facet_wrap(~entailment, scale="free_x")
```

None of these give intuitive explananations. Even if we assume the explanans and explanandum are entailed (ruling out explanantions whose explanantes and/or explananda are false), we still get "B because A" as the most probable explanation for a causal structure where there is actually no cause and where the counterfactual causal structure is B→A.

We can have the QUD be the causal structure parameters. What happens then?

```{r}
speaker1qud = function(alternatives, entailmentType, QUD) {
  rs = webppl(program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return speaker('",
                                alternatives, "', '",
                                entailmentType, "', '",
                                QUD, "');}", sep=""),
              packages = "./node_modules/jsUtils")
  rs$alternatives = alternatives
  rs$entailment = entailmentType
  rs$qud = QUD
  return(rs)
}
rs1qud = rbind(speaker1qud("only_explanations", "both", "structureParams"))
rs1qud %>% ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2)
```

OK, let's do the trick we did before and narrow down to only answers to "why A?". First without any QUD:

```{r, fig.width=10}
rs1 = rbind(
  speaker1("only_answers_to_why: A", "none"),
  speaker1("only_answers_to_why: A", "only_explanans"),
  speaker1("only_answers_to_why: A", "both"))

rs1 %>% ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2) +
  facet_wrap(~entailment, scale="free_x")
```

Wow, that's weird. When both explanans and explanandum are entailed and we restrict to "why A?", somehow "A because B" becomes the best explanation. I guess because it gives us more information about the state of the variables A and B than it does about the causal structure, and the trade-off is somehow worth it?

OK, what if we keep restricting to answers to "why A?" but we now have a QUD for structureParams?

```{r}
speaker1qud = function(alternatives, entailmentType, QUD) {
  rs = webppl(program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
              inference_opts = list(method="enumerate"),
              model_var = paste("function() {return speaker('",
                                alternatives, "', '",
                                entailmentType, "', '",
                                QUD, "');}", sep=""),
              packages = "./node_modules/jsUtils")
  rs$alternatives = alternatives
  rs$entailment = entailmentType
  rs$qud = QUD
  return(rs)
}
rs1qud = rbind(
  speaker1qud("only_answers_to_why: A", "none", "structureParams"),
  speaker1qud("only_answers_to_why: A", "only_explanans", "structureParams"),
  speaker1qud("only_answers_to_why: A", "both", "structureParams")
)
rs1qud %>% ggplot(., aes(x=support, y=prob)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  scale_fill_brewer(type="qual", palette=2) +
  facet_wrap(~entailment)
```

Seems pretty unhelpful to use the QUD here...

### Summary

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")
  
  rs = webppl(
    program_file = paste("rsa.onelink.booleans/lk",
                         story.number,
                         variable,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste(
      "function() {return speaker('only_explanations');}", sep=""),
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]
  
  return(prob_utterance)
}
expl.with.model = expl %>%
  filter(story.number %in% 1:6 & variable %in% c("A", "B", "C")) %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=story)) +
  geom_point() +
  ylim(0, 1) +
  xlim(0,1) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ggtitle("all explanations") #+ theme_black +
#theme(legend.position="right")
#ggsave("RSA-onelink.png", width=5, height=3)
cor.test(expl.with.model$model, expl.with.model$rating)
```

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")
  
  rs = webppl(
    program_file = paste("rsa.onelink.booleans/lk",
                         story.number,
                         variable,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste(
      "function() {return speaker('only_answers_to_why: ",
      explanandum,"');}", sep=""),
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]
  
  return(prob_utterance)
}
expl.with.model = expl %>%
  filter(story.number %in% 1:6 & variable %in% c("A", "B", "C")) %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=story, shape=variable)) +
  geom_point() +
  ylim(0, 1) +
  xlim(0,1) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ggtitle("with explanandum constrained") #+ theme_black +
#theme(legend.position="right")
#ggsave("RSA-onelink-explanandum-fixed.png", width=5, height=3)
# cor.test(expl.with.model$model, expl.with.model$rating)
```

```{r}
expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=tag)) +
  geom_point() +
  ylim(0, 1) +
  xlim(0,1) +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ggtitle("with explanandum constrained")
```


<!-- ## 3. Now let's introduce some BETTER causal uncertainty!

OK, but really, people are probably considering other causal configurations. They're probably not just getting rid of a particular causal link, they're probably imagining it could be in the opposite direction (e.g. B→A instead of A→B).

```{r}
# rate.expl = function(story.number, variable, actual.value,
#                      premise.variable, premise.value) {
#   
#   explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
#   explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
#   utterance = paste(explanandum, " because ", explanans, sep="")
#   
#   rs = webppl(
#     program_file = paste("rsa.fulllink.booleans/lk",
#                          story.number,
#                          "/autoexpanded.wppl", sep=""),
#     model_var = paste(
#       "function() {return speaker('only_answers_to_why: ",
#       explanandum,"');}", sep=""),
#     inference_opts = list(method="enumerate"),
#     packages = "./node_modules/jsUtils")
#   prob_utterance = rs$prob[rs$support==utterance]
#   
#   return(prob_utterance)
# }
# expl.with.model = expl %>%
#   filter(story.number %in% 1:1 & variable %in% c("A", "B")) %>%
#   mutate(model = mapply(rate.expl, story.number, variable,
#                         actual.value, premise.variable, premise.value))
# 
# expl.with.model %>% ggplot(., aes(x=model, y=rating, colour=story)) +
#   geom_point() +
#   geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
#   scale_colour_brewer(type="qual", palette = 2) +
#   ggtitle("with explanandum constrained") #+ theme_black +
# #theme(legend.position="right")
# #ggsave("RSA-fulllink-explanandum-fixed.png", width=5, height=3)
# cor.test(expl.with.model$model, expl.with.model$rating)
```

```{r}
# rs = webppl(
#   program_file = "rsa.fulllink.booleans/lk2/autoexpanded.wppl",
#   model_var = "highOrderForwardSample",
#   inference_opts = list(method="enumerate"),
#   packages = "./node_modules/jsUtils")
# 
# rs
```

-->

### LOL, I should have used a yes-no alternative utterance set

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")

  rs = webppl(
    program_file = paste("rsa.onelink.booleans/lk",
                         story.number,
                         variable,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste("function() {return speaker(':", utterance, "');}", sep=""),
    # model_var = "literal('A')",
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]

  return(prob_utterance)
}
expl.with.model = expl %>%
  filter(story.number %in% 1:6 & variable %in% c("A", "B", "C", "D")) %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>%
  # filter(as.character(variable) != as.character(premise.variable)) %>%
  ggplot(., aes(x=model, y=rating, colour=story, shape=variable)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0, 1) +
  xlim(0,1) +
  ggtitle("yes or no") #+ #theme_black +
#theme(legend.position="right")
# ggsave("RSA-onelink-yes-or-no-filtered.png", width=10, height=6)
# cor.test((expl.with.model %>% filter(abs(model-0.5)>0.001))$model, (expl.with.model %>% filter(abs(model-0.5)>0.001))$rating)
```

```{r}
expl.with.model %>%
  # filter(as.character(variable) != as.character(premise.variable)) %>%
  ggplot(., aes(x=model, y=rating, colour=tag)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0, 1) +
  xlim(0,1) +
  ggtitle("yes or no") #+ #theme_black +
#theme(legend.position="right")
ggsave("RSA-onelink-yes-or-no-filtered.png", width=10, height=6)
# cor.test((expl.with.model %>% filter(abs(model-0.5)>0.001))$model, (expl.with.model %>% filter(abs(model-0.5)>0.001))$rating)
```

Now this looks more reasonable, but there's a peak at explanations being rated at 0.5 -- probably because they convey no information, given the counterfactual causal structures considered.

Which of these are at the 0.5 mark?

Turns out most of them are the "A because A" style explanations, which because of the implicature that other explanations wouldn't work for A, would require S2.

### S2

So, let's try S2 where S1 can give any explanation for the explanandum.

So, literally, "A because A" means nothing more than "", but pragmatically, "A because A" implies that other answers to "why A?" are not good.

So I'm considering the pragmatic interpretation of "" to be the same as the literal interpretation of "", and the pragmatic interpretation of "A because A" is listener("A because A") when alternative utterances are "answers to why: A".

```{r}
# webppl(
#   program_file = "rsa.onelink.booleans/lk1A/autoexpanded.wppl",
#   model_var =
#     paste("s2('A because A', 'A')", sep=""),
#   inference_opts = list(method="enumerate"),
#   packages = "./node_modules/jsUtils"
# )
```

```{r}
rate.expl = function(story.number, variable, actual.value,
                     premise.variable, premise.value) {
  explanandum = paste(ifelse(actual.value, "", "! "), variable, sep="")
  explanans = paste(ifelse(premise.value, "! ", ""), premise.variable, sep="")
  utterance = paste(explanandum, " because ", explanans, sep="")

  rs = webppl(
    program_file = paste("rsa.onelink.booleans/lk",
                         story.number,
                         variable,
                         "/autoexpanded.wppl", sep=""),
    model_var = paste("s2('", utterance, "', '", explanandum, "', 2, 1)", sep=""),
    # model_var = "literal('A')",
    inference_opts = list(method="enumerate"),
    packages = "./node_modules/jsUtils")
  prob_utterance = rs$prob[rs$support==utterance]

  return(prob_utterance)
}
expl.with.model = expl %>%
  filter(story.number %in% 1:6 & variable %in% c("A", "B", "C", "D")) %>%
  mutate(model = mapply(rate.expl, story.number, variable,
                        actual.value, premise.variable, premise.value))
```

```{r}
expl.with.model %>%
  # filter(as.character(variable) != as.character(premise.variable)) %>%
  ggplot(., aes(x=model, y=rating, colour=story, shape=variable)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0, 1) +
  xlim(0,1) +
  theme(legend.position="right") +
  ggtitle("yes or no") #+ theme_black
ggsave("RSA-onelink-yes-or-no-s2-inneralpha2.png", width=10, height=6)
# ggsave("RSA-onelink-yes-or-no-s2.png", width=10, height=6)
# cor.test((expl.with.model %>% filter(abs(model-0.5)>0.001))$model, (expl.with.model %>% filter(abs(model-0.5)>0.001))$rating)
```

```{r}
expl.with.model %>%
  # filter(as.character(variable) != as.character(premise.variable)) %>%
  ggplot(., aes(x=model, y=rating, colour=tag)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0, 1) +
  xlim(0,1) +
  ggtitle("yes or no") + #theme_black +
#theme(legend.position="right")
# ggsave("RSA-onelink-yes-or-no-s2-tagged.png", width=10, height=6)
# cor.test((expl.with.model %>% filter(abs(model-0.5)>0.001))$model, (expl.with.model %>% filter(abs(model-0.5)>0.001))$rating)
```

```{r}
# expl.with.model %>% filter(abs(model-0.5)<0.001) %>%
#   select(premise.variable, variable, story.number)
```

<!-- ### Literal meanings of each explanation

B C 2
C D 4
C D 5

```{r}
# print_world = function(a, b, c) {
#   return(paste(
#     ifelse(a, "A", ""),
#     ifelse(b, "B", ""),
#     ifelse(c, "C", "")
#   ))
# }
# literal.model = function(story.number, utterance) {
#   words = strsplit(utterance, " ")[[1]]
#   explanandum_lst = words[1:which(words == "because")-1]
#   explanans_lst = words[(which(words == "because")+1):length(words)]
#   explanandum = paste(explanandum_lst, collapse = " ")
#   explanans = paste(explanans_lst, collapse = " ")
#   variable = explanandum_lst[(which(words == "because")-1)]
#   model_var = paste("function() {return sample(literalERP('", utterance, "'));}",
#                     sep="")
#   program_file = paste("rsa.onelink.booleans/lk",
#                        story.number,
#                        variable,
#                        "/autoexpanded.wppl", sep="")
#   rs = rbind(
#     webppl(
#       program_file = program_file,
#       model_var = model_var,
#       inference_opts = list(method="enumerate"),
#       packages = "./node_modules/jsUtils") %>%
#       mutate(utterance = utterance),
#     webppl(
#       program_file = program_file,
#       model_var = "function() {return sample(literalERP(''));}",
#       inference_opts = list(method="enumerate"),
#       packages = "./node_modules/jsUtils") %>%
#       mutate(utterance = "prior")
#   )
#   return(rs)
# }
# 
# literal.model(2, "C because B") %>%
#   mutate(world = mapply(print_world, output.A, output.B, output.C)) %>%
#   ggplot(., aes(x=world, y=prob, fill=utterance)) +
#   geom_bar(stat="identity", position="dodge") +
#   theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
#   scale_fill_brewer(type="qual", palette=2)
```
-->


## References

## Other

CF.prob model

```{r}
cf.prob = read.csv("cf-experiment-data.csv") %>%
  mutate(story = ifelse(story %in% c("story3a", "story3b"),
                        "story3", as.character(story))) %>%
  group_by(story, variable) %>%
  summarise(human.rating = mean(rating)) %>%
  as.data.frame %>%
  merge(., expl, by=c("story", "variable")) %>%
  mutate(model = ifelse(actual.value, 1-human.rating, human.rating))
cf.prob %>%
  ggplot(., aes(x=model, y=rating, colour=story, shape=variable)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0,1) +
  xlim(0,1) +
  ggtitle("CF.prob") #+ theme_black +
  #theme(legend.position="right")
# ggsave("cf-prob.png", width=5, height=3)
```

```{r}
cf.prob %>%
  mutate(tag = ifelse(
    tag %in% c("higher", "lower"),
    "miss", as.character(tag))) %>%
  ggplot(., aes(x=model, y=rating, colour=tag)) +
  geom_point() +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0) +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0,1) +
  xlim(0,1) +
  ylab("explanation rating") +
  xlab("transformed CF prob response") + theme_black +
  theme(legend.position="right")
  ggsave("cfprob-tagged.png", width=5, height=3)
```

```{r}
cf.prob %>%
  ggplot(., aes(x=model, y=rating)) +
  geom_point(colour="white") +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high), width=0, colour="white") +
  scale_colour_brewer(type="qual", palette = 2) +
  ylim(0,1) +
  xlim(0,1) +
  ylab("explanation rating") +
  xlab("transformed CF prob response") + theme_black +
  theme(legend.position="right")
  ggsave("cfprob.png", width=4, height=3)
```


