/*

Automated error analysis.

data: x1, x2, x3 -> y
We have a bunch of data points (x1, x2, y).
x1, x2 \in [0, 1].
y \in {0, 1}.
With these, we train a logistic regression.
We will simulate these data points from
	x1, x2 ~ uniform
	y ~ logistic(x1)
That is, y does not depend on x2 actually,
but it does depend on x1.

inference: learn b1, b2
Choose the MAP parameters for logistic regression.
We choose from:
	y ~ logistic(x1)
	y ~ logistic(x2)
	y ~ logistic(x1, x2)
	y ~ logistic(x2, x3)
	y ~ logistic(x1, x3)
	y ~ logistic(x1, x2, x3)

explain: misclassifications
Which term('s absense) was "responsible" for the error?

explanations: terms in the 
The agent can use the presense of absense of terms
in the regression as explanations of the misclassifications.

*/

// simulate data (R?)
// write data (R?)
// read data
// 

var data = utilities.readCSV("logistic-data.csv", {types: ['num','num','bool'], header: true}).data;
var training = data.slice(0, 50);
var test = data.slice(50, data.length);

var sigmoid = function(t) {
	return 1/(1 + Math.exp(-t));
};

var structurePrior = function() {
	uniformDraw(['b1', 'b2', 'b1+b2']);
};

// translation of: http://forestdb.org/models/logistic-regression.html
var trainLogisticRegression = cache(function(args) {
	var data = args.data;
	var structure = args.structure;

	return Infer( {method: 'SMC', particles: 100}, function() {
		var intercept = gaussian(0, 1);
		var b1 = gaussian(0, 1);
		var b2 = gaussian(0, 1);
		var variance = gamma(1, 1);

		var predict = function(d, structure) {
			if (structure=='b1') {
				return sigmoid(gaussian( intercept + b1*d.x1, variance));
			} else if (structure=='b2') {
				return sigmoid(gaussian( intercept + b2*d.x2, variance));
			} else if (structure=='b1+b2') {
				return sigmoid(gaussian( intercept + b1*d.x1 + b2*d.x2, variance));
			}
		};

		factor(0.01 * sum(map(function(d) {
			// factor by log prob of actual label
			var probTrue = predict(d, structure)
			var probLabel = d.label ? probTrue : 1-probTrue;
			return Math.log(probLabel);
		}, data)));

		return {intercept: intercept, b1: b1, b2: b2, variance: variance};
	});
});

var classifierPosterior = trainLogisticRegression({data: training, structure: 'b1+b2'});
var intercept = expectation(classifierPosterior, function(x){x.intercept});
var b1 = expectation(classifierPosterior, function(x){x.b1});
var b2 = expectation(classifierPosterior, function(x){x.b2});
var variance = expectation(classifierPosterior, function(x){x.variance});
display(intercept);
display(b1);
display(b2);
display(variance);

// var classify()

// // logisticRegression({data: data});

// display(expectation(logisticRegression({
// 	data: data,
// 	structure: 'b1'
// })));

// (define samples
//   (mh-query 

//    1000 10

//    (define (y x)
//      (gaussian (+ (* m x) b) sigma-squared))

//    (define (sigmoid x)
//      (/ 1 (+ 1 (exp (* -1 (y x))))))

//    (sigmoid 8)

//    (all
//     (map (lambda (x label) (equal? (flip (sigmoid x) label) label))
//          xs
//          labels))))

// (density samples "P(label=#t) for x=8" #t)

"finished"