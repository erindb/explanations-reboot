/*

Automated error analysis.

data: x1, x2, x3 -> y
We have a bunch of data points (x1, x2, y).
x1, x2 \in [0, 1].
y \in {0, 1}.
With these, we train a logistic regression.
We will simulate these data points from
	x1, x2 ~ uniform
	y ~ logistic(x1)
That is, y does not depend on x2 actually,
but it does depend on x1.

inference: learn b1, b2
Choose the MAP parameters for logistic regression.
We choose from:
	y ~ logistic(x1)
	y ~ logistic(x2)
	y ~ logistic(x1, x2)
	y ~ logistic(x2, x3)
	y ~ logistic(x1, x3)
	y ~ logistic(x1, x2, x3)

explain: misclassifications
Which term('s absense) was "responsible" for the error?

explanations: terms in the 
The agent can use the presense of absense of terms
in the regression as explanations of the misclassifications.

*/

// simulate data (R?)
// write data (R?)
// read data
// 

// webppl logistic.wppl --require utilities

// constants
var DATA = utilities.readCSV("logistic-data.csv", {types: ['num','num','bool','str'], header: true}).data;
var TRAIN = filter(function(d) {d.set=='train'}, DATA);
var TEST = filter(function(d) {d.set=='test'}, DATA);
var NTRAIN = TRAIN.length;
var NTEST = TEST.length;

var probability = function(dist, value) {return Math.exp(dist.score(value));};
var discreteUniform = Infer({method: 'rejection', samples: 1000}, function() {
  return uniform(0, 1);
});
var sampleWithUniformRV = function(args) {
	var prob = args.probability;
	var randomness = args.randomness;
	return randomness <= prob;
};

var independentLatentsModel = function() {
	var isTermInRegressionPrior = flip;
	var dataRandomnessPairs = map(function(i) {
		return ['r'+i, sample(discreteUniform)];
	}, utilities.range(0, NTEST));
	var allRandomness = dataRandomnessPairs.concat([
		['b1', isTermInRegressionPrior()],
		['b2', isTermInRegressionPrior()]
	]);
	return _.object(allRandomness);
};

var observablesModel = function(independentLatents) {

	var sigmoid = function(t) {
		return 1 / (1 + Math.exp(-t));
	};
	var predict = function(args) {
		var y = function(d, modelParams) {
			if (modelParams.structure == 'b1') {
				return modelParams.b1*d.x1;
			} else if (modelParams.structure == 'b2') {
				return modelParams.b2*d.x2;
			} else if (modelParams.structure == 'b1+b2') {
				return modelParams.b1*d.x1 + modelParams.b2*d.x2;
			}
			display('ERROR 14325: NOT IMPLEMENTED');
		};
		var d = args.d;
		var modelParams = args.modelParams;
		return sigmoid(gaussian(y(d, modelParams), modelParams.variance));
	};
	var fitLogisticRegression = function(includeB1, includeB2) {
		return {structure: 'b1', b1: 1, b2: null, variance: 1};
	};

	var includeB1 = independentLatents.b1;
	var includeB2 = independentLatents.b2;

	var trainedRegression = fitLogisticRegression(includeB1, includeB2);

	var observedPairs = map(function(i) {
		// predict, using randomness in independentLatents.
		var probTrue = predict({
			d: TEST[i],
			modelParams: trainedRegression
		});
		var randomness = independentLatents['r'+i];
		var classification = sampleWithUniformRV({
			probability: probTrue,
			randomness: randomness
		});
		return ['y'+i, classification];
	}, utilities.range(0, NTEST));

	var allObservables = observedPairs.concat([
		['structure', trainedRegression.structure],
		['includeB1', includeB1],
		['includeB2', includeB2]
	]);

	// return classificatins
	// and whether or not each dimension was a term in regression
	return _.object(allObservables);
};

var latents = independentLatentsModel();
var observables = observablesModel(latents);
display(observables);

// var sigmoid = function(t) {
// 	return 1/(1 + Math.exp(-t));
// };

// var trainLogisticRegression = function(args) {
// 	var training = args.training;
// 	// no inference, just return max fit for params intercept, b1, b2, and variance.
// };

// // translation of: http://forestdb.org/models/logistic-regression.html
// var trainLogisticRegression = cache(function(args) {
// 	var data = args.data;
// 	var structure = args.structure;

// 	return Optimize(function() {
// 		var intercept = gaussian(0, 1);
// 		var b1 = gaussian(0, 1);
// 		var b2 = gaussian(0, 1);
// 		var variance = gamma(1, 1);

// 		var predict = function(d, structure) {
// 			if (structure=='b1') {
// 				return sigmoid(gaussian( intercept + b1*d.x1, variance));
// 			} else if (structure=='b2') {
// 				return sigmoid(gaussian( intercept + b2*d.x2, variance));
// 			} else if (structure=='b1+b2') {
// 				return sigmoid(gaussian( intercept + b1*d.x1 + b2*d.x2, variance));
// 			}
// 		};

// 		factor(sum(map(function(d) {
// 			// factor by log prob of actual label
// 			var probTrue = predict(d, structure)
// 			var probLabel = d.label ? probTrue : 1-probTrue;
// 			return Math.log(probLabel);
// 		}, data)));

// 		// return {intercept: intercept, b1: b1, b2: b2, variance: variance};
// 	}, {optMethod: 'sgd', steps: 100});
// });

// display(trainLogisticRegression({data: data, structure: 'b1+b2'}));

// var classifierPosterior = trainLogisticRegression({data: training, structure: 'b1+b2'});
// var intercept = expectation(classifierPosterior, function(x){x.intercept});
// var b1 = expectation(classifierPosterior, function(x){x.b1});
// var b2 = expectation(classifierPosterior, function(x){x.b2});
// var variance = expectation(classifierPosterior, function(x){x.variance});
// display(intercept);
// display(b1);
// display(b2);
// display(variance);

// var classify()

// // logisticRegression({data: data});

// display(expectation(logisticRegression({
// 	data: data,
// 	structure: 'b1'
// })));

// (define samples
//   (mh-query 

//    1000 10

//    (define (y x)
//      (gaussian (+ (* m x) b) sigma-squared))

//    (define (sigmoid x)
//      (/ 1 (+ 1 (exp (* -1 (y x))))))

//    (sigmoid 8)

//    (all
//     (map (lambda (x label) (equal? (flip (sigmoid x) label) label))
//          xs
//          labels))))

// (density samples "P(label=#t) for x=8" #t)

"finished"